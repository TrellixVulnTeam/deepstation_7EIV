{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ネットワーク関数\n",
    "ReLU,CNN,weightNormalize,backwardの作成.  \n",
    "select CNN or CNNBN or CNNWN or VGGnoFC or VGGCRelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import six\n",
    "import chainer\n",
    "from chainer import cuda\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "#from chainer import initializers\n",
    "from chainer import function\n",
    "from chainer import link\n",
    "from chainer.utils import array\n",
    "from chainer.utils import type_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crelu(x):\n",
    "    h1 = F.relu(x)\n",
    "    h2 = F.relu(-x)\n",
    "    return F.concat((h1, h2), axis=1)#(input value, output value are concatenated along)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchConv2D(chainer.Chain):\n",
    "    def __init__(self, ch_in, ch_out, ksize, stride=1, pad=0, activation=F.relu):\n",
    "        super(BatchConv2D, self).__init__(\n",
    "            conv=L.Convolution2D(ch_in, ch_out, ksize, stride, pad),\n",
    "            bn=L.BatchNormalization(ch_out),\n",
    "        )\n",
    "        self.activation=activation\n",
    "\n",
    "    def __call__(self, x, train):\n",
    "        h = self.bn(self.conv(x), test=not train)\n",
    "        if self.activation is None:\n",
    "            return h\n",
    "        return self.activation(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WeightNormalize(function.Function):\n",
    "\n",
    "    \"\"\"weight normalization\"\"\"\n",
    "\n",
    "    def __init__(self, eps=1e-5):\n",
    "        self.eps = eps\n",
    "\n",
    "    def check_type_forward(self, in_types):\n",
    "        type_check.expect(in_types.size() == 2)\n",
    "        x_type, g_type = in_types\n",
    "\n",
    "        type_check.expect(\n",
    "            x_type.dtype == np.float32,\n",
    "            x_type.dtype == np.float32,\n",
    "            g_type.ndim == 0,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x, g = inputs\n",
    "        xp = cuda.get_array_module(x)\n",
    "        norm = xp.linalg.norm(x) + self.eps\n",
    "        return g * x / norm,\n",
    "\n",
    "    def backward(self, inputs, gy):\n",
    "        x, g = inputs\n",
    "        gy = gy[0]\n",
    "        xp = cuda.get_array_module(x)\n",
    "\n",
    "        norm = xp.linalg.norm(x) + self.eps\n",
    "        gg = (x * gy).sum() / norm\n",
    "        gx = g * (gy * norm - gg * x)\n",
    "        gx = gx / norm ** 2\n",
    "\n",
    "        return gx, gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_normalize(x, g, eps=1e-5):\n",
    "    return WeightNormalize(eps)(x, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WeightNormalization(link.Link):\n",
    "    def __init__(self, func, W_shape, **kwargs):\n",
    "        super(WeightNormalization, self).__init__()\n",
    "        self.func = func\n",
    "        self.kwargs = kwargs\n",
    "        self.add_param('V', W_shape, initializer=chainer.initializers.Normal(0.05))\n",
    "        self.add_uninitialized_param('g')\n",
    "        self.add_uninitialized_param('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _initialize_params(self, t):\n",
    "        xp = cuda.get_array_module(t.data)\n",
    "        mean = xp.mean(t.data, axis=(0,) + tuple(six.moves.range(2, t.ndim)))\n",
    "        std = xp.std(t.data)\n",
    "        self.add_param('g', ())\n",
    "        self.g.data[...] = 1 / std\n",
    "        self.add_param('b', (t.shape[1],))\n",
    "        self.b.data[...] = - mean / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def __call__(self, x):\n",
    "        if self.has_uninitialized_params:\n",
    "            xp = cuda.get_array_module(self.V.data)\n",
    "            W = weight_normalize(self.V, xp.asarray(1, self.V.dtype))\n",
    "            t = self.func(x, W, None, **self.kwargs)\n",
    "            self._initialize_params(t)\n",
    "        W = weight_normalize(self.V, self.g)\n",
    "        return self.func(x, W, self.b, **self.kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WNConv2D(chainer.Chain):\n",
    "    def __init__(self, ch_in, ch_out, ksize, stride=1, pad=0, activation=F.relu):\n",
    "        if hasattr(ksize, '__getitem__'):\n",
    "            kh, kw = ksize\n",
    "        else:\n",
    "            kh, kw = ksize, ksize\n",
    "        super(WNConv2D, self).__init__(\n",
    "            wn_conv=WeightNormalization(F.convolution_2d, (ch_out, ch_in, kh, kw), stride=stride, pad=pad),\n",
    "        )\n",
    "        self.activation=activation\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = self.wn_conv(x)\n",
    "        if self.activation is None:\n",
    "            return h\n",
    "        return self.activation(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CReLUBlock(chainer.Chain):\n",
    "    def __init__(self, ch_in, ch_out, ksize, stride=1, pad=0, activation=F.relu):\n",
    "        super(CReLUBlock, self).__init__(\n",
    "            conv=L.Convolution2D(ch_in, ch_out // 2, ksize, stride, pad),\n",
    "            bn=L.BatchNormalization(ch_out),\n",
    "        )\n",
    "        self.activation=activation\n",
    "\n",
    "    def __call__(self, x, train):\n",
    "        h = self.conv(x)\n",
    "        h = F.concat((h, -h), axis=1)\n",
    "        h = self.bn(h, test=not train)\n",
    "        if self.activation is None:\n",
    "            return h\n",
    "        return self.activation(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(chainer.Chain):\n",
    "    def __init__(self, ch_in, ch_out, stride=1, swapout=False, skip_ratio=0, activation1=F.relu, activation2=F.relu):\n",
    "        w = math.sqrt(2)\n",
    "        super(ResidualBlock, self).__init__(\n",
    "            conv1=L.Convolution2D(ch_in, ch_out, 3, stride, 1, w),\n",
    "            bn1=L.BatchNormalization(ch_out),\n",
    "            conv2=L.Convolution2D(ch_out, ch_out, 3, 1, 1, w),\n",
    "            bn2=L.BatchNormalization(ch_out),\n",
    "        )\n",
    "        self.activation1 = activation1\n",
    "        self.activation2 = activation2\n",
    "        self.skip_ratio = skip_ratio\n",
    "        self.swapout = swapout#実験的実装\n",
    "\n",
    "    def __call__(self, x, train):\n",
    "        skip = False\n",
    "        if train and self.skip_ratio > 0 and np.random.rand() < self.skip_ratio:\n",
    "            skip = True\n",
    "        sh, sw = self.conv1.stride\n",
    "        c_out, c_in, kh, kw = self.conv1.W.data.shape\n",
    "        b, c, hh, ww = x.data.shape\n",
    "        if sh == 1 and sw == 1:\n",
    "            shape_out = (b, c_out, hh, ww)\n",
    "        else:\n",
    "            hh = (hh + 2 - kh) // sh + 1\n",
    "            ww = (ww + 2 - kw) // sw + 1\n",
    "            shape_out = (b, c_out, hh, ww)\n",
    "        h = x\n",
    "        if x.data.shape != shape_out:\n",
    "            xp = chainer.cuda.get_array_module(x.data)\n",
    "            n, c, hh, ww = x.data.shape\n",
    "            pad_c = shape_out[1] - c\n",
    "            p = xp.zeros((n, pad_c, hh, ww), dtype=xp.float32)\n",
    "            p = chainer.Variable(p, volatile=not train)\n",
    "            x = F.concat((p, x))\n",
    "            if x.data.shape[2:] != shape_out[2:]:\n",
    "                x = F.average_pooling_2d(x, 1, 2)\n",
    "        if skip:\n",
    "            return x\n",
    "        h = self.bn1(self.conv1(h), test=not train)\n",
    "        if self.activation1 is not None:\n",
    "            h = self.activation1(h)\n",
    "        h = self.bn2(self.conv2(h), test=not train)\n",
    "        if not train:\n",
    "            h = h * (1 - self.skip_ratio)\n",
    "        if self.swapout:\n",
    "            h = F.dropout(h, train=train) + F.dropout(x, train=train)\n",
    "        else:\n",
    "            h = h + x\n",
    "        if self.activation2 is not None:\n",
    "            return self.activation2(h)\n",
    "        else:\n",
    "            return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IdentityMappingBlock(chainer.Chain):\n",
    "    def __init__(self, ch_in, ch_out, stride=1, swapout=False, skip_ratio=0, activation1=F.relu, activation2=F.relu):\n",
    "        w = math.sqrt(2)\n",
    "        super(IdentityMappingBlock, self).__init__(\n",
    "            bn1=L.BatchNormalization(ch_in),\n",
    "            conv1=L.Convolution2D(ch_in, ch_out, 3, stride, 1, w),\n",
    "            conv2=L.Convolution2D(ch_out, ch_out, 3, 1, 1, w),\n",
    "            bn2=L.BatchNormalization(ch_out),\n",
    "        )\n",
    "        self.activation1 = activation1\n",
    "        self.activation2 = activation2\n",
    "        self.swapout = swapout\n",
    "        self.skip_ratio = skip_ratio\n",
    "\n",
    "    def __call__(self, x, train):\n",
    "        skip = False\n",
    "        if train and self.skip_ratio > 0 and np.random.rand() < self.skip_ratio:\n",
    "            skip = True\n",
    "        sh, sw = self.conv1.stride\n",
    "        c_out, c_in, kh, kw = self.conv1.W.data.shape\n",
    "        b, c, hh, ww = x.data.shape\n",
    "        if sh == 1 and sw == 1:\n",
    "            shape_out = (b, c_out, hh, ww)\n",
    "        else:\n",
    "            hh = (hh + 2 - kh) // sh + 1\n",
    "            ww = (ww + 2 - kw) // sw + 1\n",
    "            shape_out = (b, c_out, hh, ww)\n",
    "        h = x\n",
    "        if x.data.shape != shape_out:\n",
    "            xp = chainer.cuda.get_array_module(x.data)\n",
    "            n, c, hh, ww = x.data.shape\n",
    "            pad_c = shape_out[1] - c\n",
    "            p = xp.zeros((n, pad_c, hh, ww), dtype=xp.float32)\n",
    "            p = chainer.Variable(p, volatile=not train)\n",
    "            x = F.concat((p, x))\n",
    "            if x.data.shape[2:] != shape_out[2:]:\n",
    "                x = F.average_pooling_2d(x, 1, 2)\n",
    "        if skip:\n",
    "            return x\n",
    "        h = self.bn1(h, test=not train)\n",
    "        if self.activation1 is not None:\n",
    "            h = self.activation1(h)\n",
    "        h = self.conv1(h)\n",
    "        h = self.bn2(h, test=not train)\n",
    "        if self.activation2 is not None:\n",
    "            h = self.activation2(h)\n",
    "        h = self.conv2(h)\n",
    "        if not train:\n",
    "            h = h * (1 - self.skip_ratio)\n",
    "        if self.swapout:\n",
    "            return F.dropout(h, train=train) + F.dropout(x, train=train)\n",
    "        else:\n",
    "            return h + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PyramidBlock(chainer.Chain):\n",
    "    def __init__(self, ch_in, ch_out, stride=1, activation=F.relu, skip_ratio=0):\n",
    "        initializer = initializers.Normal(scale=math.sqrt(2.0 / (ch_out * 3 * 3)))\n",
    "        super(PyramidBlock, self).__init__(\n",
    "            conv1=L.Convolution2D(ch_in, ch_out, 3, stride, 1, initialW=initializer),\n",
    "            conv2=L.Convolution2D(ch_out, ch_out, 3, 1, 1, initialW=initializer),\n",
    "            bn1=L.BatchNormalization(ch_in),\n",
    "            bn2=L.BatchNormalization(ch_out),\n",
    "            bn3=L.BatchNormalization(ch_out),\n",
    "        )\n",
    "        self.activation = activation\n",
    "        self.skip_ratio = skip_ratio\n",
    "\n",
    "    def __call__(self, x, train):\n",
    "        xp = chainer.cuda.get_array_module(x.data)\n",
    "        skip = False\n",
    "        if train and self.skip_ratio > 0 and np.random.rand() < self.skip_ratio:\n",
    "            skip = True\n",
    "        sh, sw = self.conv1.stride\n",
    "        c_out, c_in, kh, kw = self.conv1.W.data.shape\n",
    "        b, c, hh, ww = x.data.shape\n",
    "        if sh == 1 and sw == 1:\n",
    "            shape_out = (b, c_out, hh, ww)\n",
    "        else:\n",
    "            hh = (hh + 2 - kh) // sh + 1\n",
    "            ww = (ww + 2 - kw) // sw + 1\n",
    "            shape_out = (b, c_out, hh, ww)\n",
    "        h = x\n",
    "        if x.data.shape[2:] != shape_out[2:]:\n",
    "            x = F.average_pooling_2d(x, 1, 2)\n",
    "        if x.data.shape[1] != c_out:\n",
    "            n, c, hh, ww = x.data.shape\n",
    "            pad_c = c_out - c\n",
    "            p = xp.zeros((n, pad_c, hh, ww), dtype=xp.float32)\n",
    "            p = chainer.Variable(p, volatile=not train)\n",
    "            x = F.concat((x, p), axis=1)\n",
    "        if skip:\n",
    "            return x\n",
    "        h = self.bn1(h, test=not train)\n",
    "        h = self.conv1(h)\n",
    "        h = self.bn2(h, test=not train)\n",
    "        if self.activation is not None:\n",
    "            h = self.activation(h)\n",
    "        h = self.conv2(h)\n",
    "        h = self.bn3(h, test=not train)\n",
    "        if self.skip_ratio > 0 and not train:\n",
    "            h = h * (1 - self.skip_ratio)\n",
    "        return h + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__(\n",
    "            conv1=L.Convolution2D(3, 64, 5, stride=1, pad=2),\n",
    "            conv2=L.Convolution2D(64, 64, 5, stride=1, pad=2),\n",
    "            conv3=L.Convolution2D(64, 128, 5, stride=1,\n",
    "            pad=2),\n",
    "            l1=L.Linear(4 * 4 * 128, 1000),\n",
    "            l2=L.Linear(1000, 10),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        h1 = F.max_pooling_2d(F.relu(self.conv1(x)), 3, 2)\n",
    "        h2 = F.max_pooling_2d(F.relu(self.conv2(h1)), 3, 2)\n",
    "        h3 = F.max_pooling_2d(F.relu(self.conv3(h2)), 3, 2)\n",
    "        h4 = F.relu(self.l1(F.dropout(h3, train=train)))\n",
    "        return self.l2(F.dropout(h4, train=train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-12-f01cf5b01763>, line 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-f01cf5b01763>\"\u001b[0;36m, line \u001b[0;32m50\u001b[0m\n\u001b[0;31m    def __init__(self, ch_in, ch_out, stride=1, swapout=False, skip_ratio=0, activation1=F.relu, activation2=F.relu):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class CNNBN(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(CNNBN, self).__init__(\n",
    "            bconv1=BatchConv2D(3, 64, 5, stride=1, pad=2),\n",
    "            bconv2=BatchConv2D(64, 64, 5, stride=1, pad=2),\n",
    "            bconv3=BatchConv2D(64, 128, 5, stride=1, pad=2),\n",
    "            l1=L.Linear(4 * 4 * 128, 1000),\n",
    "            l2=L.Linear(1000, 10),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        h1 = F.max_pooling_2d(self.bconv1(x, train), 3, 2)\n",
    "        h2 = F.max_pooling_2d(self.bconv2(h1, train), 3, 2)\n",
    "        h3 = F.max_pooling_2d(self.bconv3(h2, train), 3, 2)\n",
    "        h4 = F.relu(self.l1(F.dropout(h3, train=train)))\n",
    "return self.l2(F.dropout(h4, train=train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-09008ec207b2>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-09008ec207b2>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    h = self.bconv3_4(h, train)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class CNNWN(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(CNNWN, self).__init__(\n",
    "            wn_conv1=WNConv2D(3, 64, 5, stride=1, pad=2),\n",
    "            wn_conv2=WNConv2D(64, 64, 5, stride=1, pad=2),\n",
    "     nvconv3_3(h, train)\n",
    "        h = self.bconv3_4(h, train)\n",
    "        h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = F.relu(self.fc4(F.dropout(h, train=train)))\n",
    "        h = F.relu(self.fc5(F.dropout(h, train=train)))\n",
    "        h = self.fc6(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VGG(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__(\n",
    "            bconv1_1=BatchConv2D(3, 64, 3, stride=1, pad=1),\n",
    "            bconv1_2=BatchConv2D(64, 64, 3, stride=1, pad=1),\n",
    "            bconv2_1=BatchConv2D(64, 128, 3, stride=1, pad=1),\n",
    "            bconv2_2=BatchConv2D(128, 128, 3, stride=1, pad=1),\n",
    "            bconv3_1=BatchConv2D(128, 256, 3, stride=1, pad=1),\n",
    "            bconv3_2=BatchConv2D(256, 256, 3, stride=1, pad=1),\n",
    "            bconv3_3=BatchConv2D(256, 256, 3, stride=1, pad=1),\n",
    "            bconv3_4=BatchConv2D(256, 256, 3, stride=1, pad=1),\n",
    "            fc4=L.Linear(4 * 4 * 256, 1024),\n",
    "            fc5=L.Linear(1024, 1024),\n",
    "            fc6=L.Linear(1024, 10),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        h = self.bconv1_1(x, train)\n",
    "        h = self.bconv1_2(h, train)\n",
    "        h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = self.bconv2_1(h, train)\n",
    "        h = self.bconv2_2(h, train)\n",
    "        h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = self.bconv3_1(h, train)\n",
    "        h = self.bconv3_2(h, train)\n",
    "        h = self.bconv3_3(h, train)\n",
    "        h = self.bconv3_4(h, train)\n",
    "        h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = F.relu(self.fc4(F.dropout(h, train=train)))\n",
    "        h = F.relu(self.fc5(F.dropout(h, train=train)))\n",
    "        h = self.fc6(h)\n",
    "return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-c837d8a7366a>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-c837d8a7366a>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    h = self.bconv2_1(h, train)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class VGGNoFC(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__(\n",
    "            bconv1_1=BatchConv2D(3, 64, 3, stride=1, pad=1),\n",
    "            bconv1_2=BatchConv2D(64, 64, 3, stride=1, pad=1),\n",
    "            bconv2_1=BatchConv2D(64, 128, 3, stride=1, pad=1),\n",
    "            bconv2_2=BatchConv2D(128, 128, 3, stride=1, pad=1),\n",
    "            bconv3_1=BatchConv2D(128, 256,         h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = self.bconv2_1(h, train)\n",
    "        h = self.bconv2_2(h, train)\n",
    "        h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = self.bconv3_1(h, train)\n",
    "        h = self.bconv3_2(h, train)\n",
    "        h = self.bconv3_3(h, train)\n",
    "        h = self.bconv3_4(h, train)\n",
    "        h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = F.average_pooling_2d(h, 4, 1, 0)\n",
    "     D(256, 256, 3, stride=1, pad=1),\n",
    "            bconv2_4=BatchConv2D(256, 256, 3, stride=1, pad=1),\n",
    "            bconv3_1=BatchConv2D(256, 512, 3, stride=1, pad=1),\n",
    "            bconv3_2=BatchConv2D(512, 512, 3, stride=1, pad=1),\n",
    "            bconv3_3=BatchConv2D(512, 512, 3, stride=1, pad=1),\n",
    "            bconv3_4=BatchConv2D(512, 512, 3, stride=1, pad=1),\n",
    "            bconv3_5=BatchConv2D(512, 512, 3, stride=1, pad=1),\n",
    "           rain=train)\n",
    "        h = self.bconv2_1(h, train)\n",
    "        h = F.dropout(h, 0.25, train=train)\n",
    "        h = self.bconv2_2(h, train)\n",
    "        h = F.droh = F.average_pooling_2d(h, 4, 1, 0)\n",
    "        h = self.fc(F.dropout(h, train=trai0], 1, stride=1, pad=0),\n",
    "            conv6_2=BatchConv2D(out_ch[2][0], out_ch[2][1], 3, stride=1, pad=1),\n",
    "            conv6_3, (32, (16, 32), (16, 32, 32), 64)),\n",
    "            l1_2=InceptionBlock(64, (32, (16, 32), (16, 32, 32), 64)),\n",
    "            l2_1=InceptionBlock(64, (64, (32, 64), (32, 64, 64), 128)),\n",
    "            l2_2=InceptionBlock(128, (64, (32, 64), (32, 64, 64), 128)),\n",
    "            l3_1=InceptionBlock(128, (128, (64, 128), (64, 128, 128), 256)),\n",
    "            l3_2=InceptionBlock(256, (128, (64, 128), (64, 128, 128), 256)),\n",
    "            l4_1=InceptionBlock(256, (128, (64, 128), (64, 128, 128), 256)),\n",
    "            l4_2=InceptionBlock(256, (128, (64, 12, train=train)\n",
    "        h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = self.l4_1(h, train=train)\n",
    "        h = self.l4_2(h, train=train)\n",
    "        h = F.dropout(h, 0.25, train=train)\n",
    "        h = F.average_pooling_2d(h, 4, 1, 0)\n",
    "        h = self.fc(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VGGWide(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(VGG2, self).__init__(\n",
    "            bconv1_1=BatchConv2D(3, 128, 3, stride=1, pad=1),\n",
    "            bconv1_2=BatchConv2D(128, 128, 3, stride=1, pad=1),\n",
    "            bconv1_3=BatchConv2D(128, 128, 3, stride=1, pad=1),\n",
    "            bconv1_4=BatchConv2D(128, 128, 3, stride=1, pad=1),\n",
    "            bconv2_1=BatchConv2D(128, 256, 3, stride=1, pad=1),\n",
    "            bconv2_2=BatchConv2D(256, 256, 3, stride=1, pad=1),\n",
    "            bconv2_3=BatchConv2D(256, 256, 3, stride=1, pad=1),\n",
    "            bconv2_4=BatchConv2D(256, 256, 3, stride=1, pad=1),\n",
    "            bconv3_1=BatchConv2D(256, 512, 3, stride=1, pad=1),\n",
    "            bconv3_2=BatchConv2D(512, 512, 3, stride=1, pad=1),\n",
    "            bconv3_3=BatchConv2D(512, 512, 3, stride=1, pad=1),\n",
    "            bconv3_4=BatchConv2D(512, 512, 3, stride=1, pad=1),\n",
    "            bconv3_5=BatchConv2D(512, 512, 3, stride=1, pad=1),\n",
    "            bconv3_6=BatchConv2D(512, 512, 3, stride=1, pad=1),\n",
    "            bconv3_7=BatchConv2D(512, 512, 3, stride=1, pad=1),\n",
    "            bconv3_8=BatchConv2D(512, 512, 3, stride=1, pad=1),\n",
    "            fc=F.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        h = self.bconv1_1(x, train)\n",
    "        h = F.dropout(h, 0.25, train=train)\n",
    "        h = self.bconv1_2(h, train)\n",
    "        h = F.dropout(h, 0.25, train=train)\n",
    "        h = self.bconv1_3(h, train)\n",
    "        h = F.dropout(h, 0.25, train=train)\n",
    "        h = self.bconv1_4(h, train)\n",
    "        h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = self.bconv2_1(h, train)\n",
    "        h = F.dropout(h, 0.25, train=train)\n",
    "        h = self.bconv2_2(h, train)\n",
    "        h = F.dropout(h, 0.25, train=train)\n",
    "        h = self.bconv2_3(h, train)\n",
    "        h = F.dropout(h, 0.25, train=train)\n",
    "        h = self.bconv2_4(h, train)\n",
    "        h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = self.bconv3_1(h, train)\n",
    "        h = F.dropout(h, 0.25, train=train)\n",
    "        h = self.bconv3_2(h, train)\n",
    "        h = F.dropout(h, 0.25, train=train)\n",
    "        h = self.bconv3_3(h, train)\n",
    "        h = F.dropout(h, 0.25, train=train)\n",
    "        h = self.bconv3_4(h, train)\n",
    "        h = F.dropout(h, 0.25, train=train)\n",
    "        h = self.bconv3_5(h, train)\n",
    "        h = F.dropout(h, 0.25, train=train)\n",
    "        h = self.bconv3_6(h, train)\n",
    "        h = F.dropout(h, 0.25, train=train)\n",
    "        h = self.bconv3_7(h, train)\n",
    "        h = F.dropout(h, 0.25, train=train)\n",
    "        h = self.bconv3_8(h, train)\n",
    "        h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = F.average_pooling_2d(h, 4, 1, 0)\n",
    "        h = self.fc(F.dropout(h, train=train))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InceptionBlock(chainer.Chain):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        conv_in_ch = (out_ch[0] + out_ch[1][1] + out_ch[2][2]) * 2\n",
    "        super(InceptionBlock, self).__init__(\n",
    "            conv1_1=BatchConv2D(in_ch, out_ch[0], 1, stride=1, pad=0),\n",
    "            conv2_1=BatchConv2D(in_ch, out_ch[1][0], 1, stride=1, pad=0),\n",
    "            conv2_2=BatchConv2D(out_ch[1][0], out_ch[1][1], 3, stride=1, pad=1),\n",
    "            conv3_1=BatchConv2D(in_ch, out_ch[2][0], 1, stride=1, pad=0),\n",
    "            conv3_2=BatchConv2D(out_ch[2][0], out_ch[2][1], 3, stride=1, pad=1),\n",
    "            conv3_3=BatchConv2D(out_ch[2][1], out_ch[2][2], 3, stride=1, pad=1),\n",
    "            conv4_1=BatchConv2D(in_ch, out_ch[0], 1, stride=1, pad=0),\n",
    "            conv5_1=BatchConv2D(in_ch, out_ch[1][0], 1, stride=1, pad=0),\n",
    "            conv5_2=BatchConv2D(out_ch[1][0], out_ch[1][1], 3, stride=1, pad=1),\n",
    "            conv6_1=BatchConv2D(in_ch, out_ch[2][0], 1, stride=1, pad=0),\n",
    "            conv6_2=BatchConv2D(out_ch[2][0], out_ch[2][1], 3, stride=1, pad=1),\n",
    "            conv6_3=BatchConv2D(out_ch[2][1], out_ch[2][2], 3, stride=1, pad=1),\n",
    "            conv=BatchConv2D(conv_in_ch, out_ch[3], 1, stride=1, pad=0, activation=None),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        h1 = self.conv1_1(x, train=train)\n",
    "        h2 = self.conv2_2(self.conv2_1(x, train=train), train=train)\n",
    "        h3 = self.conv3_3(self.conv3_2(self.conv3_1(x, train=train), train=train), train=train)\n",
    "        h4 = self.conv4_1(x, train=train)\n",
    "        h5 = self.conv5_2(self.conv5_1(x, train=train), train=train)\n",
    "        h6 = self.conv6_3(self.conv6_2(self.conv6_1(x, train=train), train=train), train=train)\n",
    "        h = F.concat((h1, h2, h3, h4, h5, h6), axis=1)\n",
    "        return self.conv(h, train=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Inception(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(Inception, self).__init__(\n",
    "            l0=BatchConv2D(3, 64, 5, stride=1, pad=2),\n",
    "            l1_1=InceptionBlock(64, (32, (16, 32), (16, 32, 32), 64)),\n",
    "            l1_2=InceptionBlock(64, (32, (16, 32), (16, 32, 32), 64)),\n",
    "            l2_1=InceptionBlock(64, (64, (32, 64), (32, 64, 64), 128)),\n",
    "            l2_2=InceptionBlock(128, (64, (32, 64), (32, 64, 64), 128)),\n",
    "            l3_1=InceptionBlock(128, (128, (64, 128), (64, 128, 128), 256)),\n",
    "            l3_2=InceptionBlock(256, (128, (64, 128), (64, 128, 128), 256)),\n",
    "            l4_1=InceptionBlock(256, (128, (64, 128), (64, 128, 128), 256)),\n",
    "            l4_2=InceptionBlock(256, (128, (64, 128), (64, 128, 128), 256)),\n",
    "            fc=L.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        h = self.l0(x, train=train)\n",
    "        h = self.l1_1(h, train=train)\n",
    "        h = self.l1_2(h, train=train)\n",
    "        h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = self.l2_1(h, train=train)\n",
    "        h = self.l2_2(h, train=train)\n",
    "        h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = self.l3_1(h, train=train)\n",
    "        h = self.l3_2(h, train=train)\n",
    "        h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = self.l4_1(h, train=train)\n",
    "        h = self.l4_2(h, train=train)\n",
    "        h = F.dropout(h, 0.25, train=train)\n",
    "        h = F.average_pooling_2d(h, 4, 1, 0)\n",
    "        h = self.fc(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VGGCReLU(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(CReLUVGG, self).__init__(\n",
    "            bconv1_1=CReLUBlock(3, 64, 3, stride=1, pad=1),\n",
    "            bconv1_2=CReLUBlock(64, 64, 3, stride=1, pad=1),\n",
    "            bconv2_1=CReLUBlock(64, 128, 3, stride=1, pad=1),\n",
    "            bconv2_2=CReLUBlock(128, 128, 3, stride=1, pad=1),\n",
    "            bconv3_1=CReLUBlock(128, 256, 3, stride=1, pad=1),\n",
    "            bconv3_2=CReLUBlock(256, 256, 3, stride=1, pad=1),\n",
    "            bconv3_3=CReLUBlock(256, 256, 3, stride=1, pad=1),\n",
    "            bconv3_4=CReLUBlock(256, 256, 3, stride=1, pad=1),\n",
    "            fc4=L.Linear(4 * 4 * 256, 1024),\n",
    "            fc5=L.Linear(1024, 1024),\n",
    "            fc6=L.Linear(1024, 10),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        h = self.bconv1_1(x, train)\n",
    "        h = self.bconv1_2(h, train)\n",
    "        h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = self.bconv2_1(h, train)\n",
    "        h = self.bconv2_2(h, train)\n",
    "        h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = self.bconv3_1(h, train)\n",
    "        h = self.bconv3_2(h, train)\n",
    "        h = self.bconv3_3(h, train)\n",
    "        h = self.bconv3_4(h, train)\n",
    "        h = F.dropout(F.max_pooling_2d(h, 2), 0.25, train=train)\n",
    "        h = F.relu(self.fc4(F.dropout(h, train=train)))\n",
    "        h = F.relu(self.fc5(F.dropout(h, train=train)))\n",
    "        h = self.fc6(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResidualNet(chainer.Chain):\n",
    "    def __init__(self, depth=18, swapout=False, skip=False):\n",
    "        super(ResidualNet, self).__init__()\n",
    "        links = [('bconv1', BatchConv2D(3, 16, 3, 1, 1), True)]\n",
    "        skip_size = depth * 3 - 3\n",
    "        for i in six.moves.range(depth):\n",
    "            if skip:\n",
    "                skip_ratio = float(i) / skip_size * 0.5\n",
    "            else:\n",
    "                skip_ratio = 0\n",
    "            links.append(('res{}'.format(len(links)), ResidualBlock(16, 16, swapout=swapout, skip_ratio=skip_ratio, ), True))\n",
    "        links.append(('res{}'.format(len(links)), ResidualBlock(16, 32, stride=2, swapout=swapout), True))\n",
    "        for i in six.moves.range(depth - 1):\n",
    "            if skip:\n",
    "                skip_ratio = float(i + depth) / skip_size * 0.5\n",
    "            else:\n",
    "                skip_ratio = 0\n",
    "            links.append(('res{}'.format(len(links)), ResidualBlock(32, 32, swapout=swapout, skip_ratio=skip_ratio), True))\n",
    "        links.append(('res{}'.format(len(links)), ResidualBlock(32, 64, stride=2, swapout=swapout), True))\n",
    "        for i in six.moves.range(depth - 1):\n",
    "            if skip:\n",
    "                skip_ratio = float(i + depth * 2 - 1) / skip_size * 0.5\n",
    "            else:\n",
    "                skip_ratio = 0\n",
    "            links.append(('res{}'.format(len(links)), ResidualBlock(64, 64, swapout=swapout, skip_ratio=skip_ratio), True))\n",
    "        links.append(('_apool{}'.format(len(links)), F.AveragePooling2D(8, 1, 0, False, True), False))\n",
    "        links.append(('fc{}'.format(len(links)), L.Linear(64, 10), False))\n",
    "\n",
    "        for name, f, _with_train in links:\n",
    "            if not name.startswith('_'):\n",
    "                self.add_link(*(name, f))\n",
    "        self.layers = links\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        h = x\n",
    "        for name, f, with_train in self.layers:\n",
    "            if with_train:\n",
    "                h = f(h, train=train)\n",
    "            else:\n",
    "                h = f(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IdentityMapping(chainer.Chain):\n",
    "    def __init__(self, depth=18, swapout=False, skip=False):\n",
    "        super(IdentityMapping, self).__init__()\n",
    "        links = [('bconv1', BatchConv2D(3, 16, 3, 1, 1), True)]\n",
    "        skip_size = depth * 3 - 3\n",
    "        for i in six.moves.range(depth):\n",
    "            if skip:\n",
    "                skip_ratio = float(i) / skip_size * 0.5\n",
    "            else:\n",
    "                skip_ratio = 0\n",
    "            links.append(('res{}'.format(len(links)), IdentityMappingBlock(16, 16, swapout=swapout, skip_ratio=skip_ratio, activation1=F.relu, activation2=F.relu), True))\n",
    "        links.append(('res{}'.format(len(links)), IdentityMappingBlock(16, 32, stride=2, swapout=swapout, activation1=F.relu, activation2=F.relu), True))\n",
    "        for i in six.moves.range(depth - 1):\n",
    "            if skip:\n",
    "                skip_ratio = float(i + depth) / skip_size * 0.5\n",
    "            else:\n",
    "                skip_ratio = 0\n",
    "            links.append(('res{}'.format(len(links)), IdentityMappingBlock(32, 32, swapout=swapout, skip_ratio=skip_ratio, activation1=F.relu, activation2=F.relu), True))\n",
    "        links.append(('res{}'.format(len(links)), IdentityMappingBlock(32, 64, stride=2, swapout=swapout, activation1=F.relu, activation2=F.relu), True))\n",
    "        for i in six.moves.range(depth - 1):\n",
    "            if skip:\n",
    "                skip_ratio = float(i + depth * 2 - 1) / skip_size * 0.5\n",
    "            else:\n",
    "                skip_ratio = 0\n",
    "            links.append(('res{}'.format(len(links)), IdentityMappingBlock(64, 64, swapout=swapout, skip_ratio=skip_ratio, activation1=F.relu, activation2=F.relu), True))\n",
    "        links.append(('_apool{}'.format(len(links)), F.AveragePooling2D(8, 1, 0, False, True), False))\n",
    "        links.append(('fc{}'.format(len(links)), L.Linear(64, 10), False))\n",
    "\n",
    "        for name, f, _with_train in links:\n",
    "            if not name.startswith('_'):\n",
    "                self.add_link(*(name, f))\n",
    "        self.layers = links\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        h = x\n",
    "        for name, f, with_train in self.layers:\n",
    "            if with_train:\n",
    "                h = f(h, train=train)\n",
    "            else:\n",
    "                h = f(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-dbbea03de7c9>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-dbbea03de7c9>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    bconv1_2=CReLUBlock(6_(self, x, train=True):\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class PyramidNet(chainer.Chain):\n",
    "    def __init__(self, depth=18, alpha=16, start_channel=16, skip=False):\n",
    "        super(PyramidNet, self).__init__()\n",
    "        channel_diff = float(alpha) / depth\n",
    "        channel = start_channel\n",
    "        links = [('bconv1', BatchConv2D(3, channel, 3, 1, 1), True, False)]\n",
    "        skip_size = depth * 3 - 3\n",
    "        for i in six.moves.range(depth):\n",
    "            if skip:\n",
    "                skip_ratio = float(i) / skip_size * 0.5\n",
    "            else:\n",
    "                skip_ratio = 0\n",
    "            in_channel = channel\n",
    "            channel += channel_diff\n",
    "            links.append(('py{}'.format(len(links)), PyramidBlock(int(round(in_channel)), int(round(channel)),  skip_ratio=skip_ratio), True, False))\n",
    "        in_channel = channel\n",
    "        channel += channel_diff\n",
    "        links.append(('py{}'.format(len(links)), PyramidBlock(int(round(in_channel)), int(round(channel)), stride=2), True, False))\n",
    "        for i in six.moves.range(depth - 1):\n",
    "            if skip:\n",
    "                skip_ratio = float(i + depth) / skip_size * 0.5\n",
    "            else:\n",
    "                skip_ratio = 0\n",
    "            in_channel = channel\n",
    "            channel += channel_diff\n",
    "            links.append(('py{}'.format(len(links)), PyramidBlock(int(round(in_channel)), int(round(channel)),  skip_ratio=skip_ratio), True, False))\n",
    "        in_channel = channel\n",
    "        channel += channel_diff\n",
    "        links.append(('py{}'.format(len(links)), PyramidBlock(int(round(in_channel)), int(round(channel)), stride=2), True, False))\n",
    "        for i in six.moves.range(depth - 1):\n",
    "            if skip:\n",
    "                skip_ratio = float(i + depth * 2 - 1) / skip_size * 0.5\n",
    "            else:\n",
    "                skip_ratio = 0\n",
    "            in_channel = channel\n",
    "            channel += channel_diff\n",
    "            links.append(('py{}'.format(len(links)), PyramidBlock(int(round(in_channel)), int(round(channel)),  skip_ratio=skip_ratio), True, False))\n",
    "        links.append(('bn{}'.format(len(links)), L.BatchNormalization(int(round(channel))), False, True))\n",
    "        links.append(('_relu{}'.format(len(links)), F.ReLU(), False, False))\n",
    "        links.append(('_apool{}'.format(len(links)), F.AveragePooling2D(8, 1, 0, False, True), False, False))\n",
    "        links.append(('fc{}'.format(len(links)), L.Linear(int(round(channel)), 10), False, False))\n",
    "\n",
    "        for name, f, _with_train, _with_test in links:\n",
    "            if not name.startswith('_'):\n",
    "                self.add_link(*(name, f))\n",
    "        self.layers = links\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        h = x\n",
    "        for name, f, with_train, with_test in self.layers:\n",
    "            if with_train:\n",
    "                h = f(h, train=train)\n",
    "            elif with_test:\n",
    "                h = f(h, test=not train)\n",
    "            else:\n",
    "                h = f(h)\n",
    "return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
